{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, wait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Data dạng folder chứa ảnh hoặc âm thanh , text thường có 2 cách load :\n",
    "\n",
    "   ### Load thẳng dữ liệu ảnh , âm thanh hoặc text vào biến chẳng hạn X_train , Y_train \n",
    "        + ưu nhược điêm : \n",
    "            - Phụ thuộc số lượng file , kích thước mỗi điểm data\n",
    "             \n",
    "             \n",
    "   ### Load theo batch :\n",
    "        + Đến batch nào train thì sẽ load data của batch đấy \n",
    "    \n",
    "    - VD :\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epoch = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_data(file):\n",
    "    return np.random.rand((200,161))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_data_full(desc_file):\n",
    "    data_audio, texts = [], []\n",
    "    with open(desc_file) as json_line_file:\n",
    "        for line_num, json_line in enumerate(json_line_file):\n",
    "            try:\n",
    "                spec = json.loads(json_line)\n",
    "                data_audio = process_data(spec['key'])\n",
    "                texts.append(spec['text'])\n",
    "            except:\n",
    "                pass\n",
    "    return data_audio, texts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def next_batch_1(batch_size,data_x,data_y,start):\n",
    "    x = data_x[start: start + batch_size]\n",
    "    y = data_y[start: start + batch_size]\n",
    "    return x,y\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "X_train , Y_train = load_data_full('data_small.json')\n",
    "num_batch = len(X_train) // batch_size\n",
    "\n",
    "sess= tf.Session()\n",
    "\n",
    "for i in range(epoch):\n",
    "    start = 0\n",
    "    for idx in range(num_batch):\n",
    "        x, y = next_batch_1(batch_size,X_train,Y_train,start)\n",
    "        start += batch_size\n",
    "        sess.run([],feed_dict={......})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Nhược điểm : \n",
    "    nếu data X_train ,  Y_train rất lớn sẽ dẫn đến tràn RAM , load chậm ... thời gian bốc batch lâu\n",
    "    \n",
    "\n",
    "Viết hàm đơn lẻ này k tốt vì phải truyền nhiều tham số  và quản lí giá trị các tham số truyền vào \n",
    "\n",
    "\n",
    "---->> Viết 1 Class cho việc đọc data đồng thời thay đổi cách đọc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self,desc_file,batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.desc_file = desc_file\n",
    "        self.start = 0\n",
    "        \n",
    "    def load_data(self):\n",
    "        audio_paths,  texts = [], [], []\n",
    "        with open(self.desc_file) as json_line_file:\n",
    "            for line_num, json_line in enumerate(json_line_file):\n",
    "                try:\n",
    "                    spec = json.loads(json_line)\n",
    "                    audio_paths.append(spec['key'])\n",
    "                    texts.append(spec['text'])\n",
    "                except:\n",
    "                    pass\n",
    "        self.paths = audio_paths\n",
    "        sefl.texts = durations\n",
    "        self.num_batch = len(audio_paths) // self.batch_size\n",
    "\n",
    "    def next_batch_2(batch_size,data_x,data_y,end_epoch):\n",
    "        \n",
    "        x_paths = data_x[self.start: self.start + batch_size]\n",
    "        \n",
    "        x = [process_data(p) for p in x_paths]\n",
    "        y = data_y[self.start: self.start + batch_size]\n",
    "        self.start += self.batch_size\n",
    "        if end_epoch:\n",
    "            self.start = 0\n",
    "            \n",
    "        return x,y\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Code phần mới ta sẽ chỉ xử lí data theo từng batch : hàm process_data sẽ tạo ra data cần thiết cho mình dựa trên path\n",
    "   Tuy nhiên vẫn phải quản lí batch data dựa theo chỉ số  start và end của epoch \n",
    "   -------- \n",
    "    Hàm next batch mới \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "    def next_batch_2(self):\n",
    "        idx = 0\n",
    "        start = 0\n",
    "        while(idx < self.num_batch):\n",
    "            x_paths = self.paths[start: start + self.batch_size]\n",
    "        \n",
    "            x = [process_data(p) for p in x_paths]\n",
    "            y = self.texts[start: start + self.batch_size]\n",
    "            idx += 1\n",
    "            start += self.batch_size\n",
    "            yield x, y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "dataset = Dataset(batch_size=32,desc_file='data_small.json')\n",
    "num_batch = dataset.num_batch\n",
    "\n",
    "sess= tf.Session()\n",
    "\n",
    "for i in range(epoch):\n",
    "    start = 0\n",
    "    for idx in range(datnum_batch):\n",
    "        x, y = dataset.next_batch_2()\n",
    "        \n",
    "        sess.run([],feed_dict={......})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Như vậy với mỗi iter model sẽ đợi get 1 batch ra để đưa vào run : \n",
    "\n",
    "##  Đã tối ưu chưa ? \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Việc đọc data còn có thể được tốí ưu hơn nữa vơí thread của python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "    def prepare_data(paths,texts):\n",
    "        x = [process_data(p) for p in paths]\n",
    "        y = texts\n",
    "        return {'x' : x, 'y' : y}\n",
    "    \n",
    "    def next_batch(self):\n",
    "        pool = ThreadPoolExecutor(1)\n",
    "        future = pool.submit(self.prepare_data,self.paths[:self.batch_size]\n",
    "                             ,self.texts[:self.batch_size])\n",
    "        start = self.batch_size\n",
    "        for i in range(self.num_batch - 1):\n",
    "            wait([future])\n",
    "            minibatch = future.result()\n",
    "            # While the current minibatch is being consumed, prepare the next\n",
    "            future = pool.submit(self.prepare_data,self.paths[start:self.batch_size]\n",
    "                             ,self.texts[start:self.batch_size])\n",
    "            yield minibatch\n",
    "            start += minibatch_size\n",
    "        # Wait on the last minibatch\n",
    "        wait([future])\n",
    "        minibatch = future.result()\n",
    "        yield minibatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Multi-Gpu with Keras  backend Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "About Keras  :  Cung cấp 1 higher API cho tensorflow thông qua Model và K.function \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Bidirectional(\n",
    "        LSTM(units=100, return_sequences=True,\n",
    "            activation='tanh', dropout=0.2,recurrent_dropout=0.5),\n",
    "        input_shape=(self.sent_length, self.width),\n",
    "        merge_mode='concat'\n",
    "    ))\n",
    "\n",
    "    model.add(Bidirectional(\n",
    "        LSTM(units=100, return_sequences=True,\n",
    "            activation='tanh', dropout=0.2,recurrent_dropout=0.5),\n",
    "        merge_mode='concat'\n",
    "    ))\n",
    "    model.add(TimeDistributed(\n",
    "            Dense(self.num_of_class, activation='softmax')))\n",
    "            \n",
    "    optimizer = Adam(lr=1e-3,decay=1e-8)        \n",
    "    model.compile(loss=....,optimizer=optimizer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "About Tensoflow : \n",
    "    Cung cấp định nghĩa vùng tính toán qua câu lệnh:\n",
    "        with tf.device(\"....\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "  c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Giới hạn bộ nhớ sử dụng cua GPU\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Có 2 kiểu chính để sử dụng multi-gpu của Tensorflow + Keras:\n",
    "      \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# P1:\n",
    "\n",
    "     Tensor input của model sẽ được slice làm n phần tương ứng n-gpus --> \n",
    "     tính được n-output của model và gộp lại thành 1 output --->\n",
    "     Tính loss trên output này ... update model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# P2: \n",
    "\n",
    "    Với mỗi gpu get ra 1 batch input vào model -> output \n",
    "    Tính loss + gradients trên output này \n",
    "    Tính average gradients của các output ---> sử dụng gradients này làm update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import keras.layers as L\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras.layers import merge, Concatenate,Flatten\n",
    "from keras.models import Model,Sequential\n",
    "from keras.layers import (BatchNormalization, Convolution1D,Convolution2D, Dense,\n",
    "                          Input, GRU,LSTM, Bidirectional, TimeDistributed,Lambda,Reshape, Activation,Dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_dim = 161\n",
    "output_dim = 91\n",
    "acoustic_input = Input(shape=(None, input_dim,1), name='acoustic_input')\n",
    "conv2d_1 = Convolution2D(32, kernel_size = (11,41), strides=(2, 2), padding='valid',\n",
    "                        data_format=\"channels_last\",activation='relu')(acoustic_input)\n",
    "conv2d_1 = BatchNormalization(name='bn_conv1_2d')(conv2d_1)\n",
    "\n",
    "\n",
    "\n",
    "conv2d_2 = Convolution2D(32, kernel_size = (11,21),strides=(1, 2), padding='valid',\n",
    "                        data_format=\"channels_last\",activation='relu')(conv2d_1)\n",
    "conv2d_2= BatchNormalization(name='bn_conv2_2d')(conv2d_2)\n",
    "\n",
    "network_output = Dense(output_dim,activation=\"softmax\",name=\"output\")(conv2d_2)\n",
    "model = Model(inputs=acoustic_input,outputs=network_output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#code for multi gpu Keras\n",
    "\n",
    "def make_parallel(model, gpu_count):\n",
    "    \"\"\" Allows a model to run on multiple GPUs.\n",
    "    \"\"\"\n",
    "    def slice_batch(x, n_gpus, part):\n",
    "        \n",
    "        sh = K.shape(x)\n",
    "        L = sh[0] // n_gpus # sh[0] = batch_size\n",
    "\n",
    "        if part == n_gpus - 1:\n",
    "            return x[part*L:]\n",
    "\n",
    "        return x[part*L:(part+1)*L]\n",
    "\n",
    "    all_outputs = []\n",
    "\n",
    "    # Empty list for each output in the model\n",
    "    for i in range(len(model.outputs)):\n",
    "        all_outputs.append([])\n",
    "\n",
    "    # Place a copy of the model on each GPU, \n",
    "    # each getting a slice of the batch\n",
    "    with tf.variable_scope(tf.get_variable_scope()):\n",
    "        \n",
    "        for i in range(gpu_count):\n",
    "\n",
    "            with tf.device('/cpu:0'):\n",
    "                slices = []  # multi-input case\n",
    "                for x in model.inputs:\n",
    "                    input_shape = (None,) + tuple(x.get_shape().as_list())[1:]\n",
    "                    slice_g = Lambda(\n",
    "                        slice_batch,  # lambda shape: shape,\n",
    "                        lambda shape: input_shape,arguments={'n_gpus': gpu_count, 'part': i})(x)\n",
    "                    slices.append(slice_g)\n",
    "\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                with tf.name_scope('tower_%d' % i) as scope:\n",
    "\n",
    "                    outputs = model(slices)\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "                    if not isinstance(outputs, list):\n",
    "                        outputs = [outputs]\n",
    "\n",
    "                    # Save all the outputs for \n",
    "                    # merging back together later\n",
    "                    for l in range(len(outputs)):\n",
    "                        all_outputs[l].append(outputs[l])\n",
    "\n",
    "    # Merge outputs on CPU\n",
    "    with tf.device('/cpu:0'):\n",
    "        merged = []\n",
    "\n",
    "        func = lambda x: L.concatenate(x, axis=0)\n",
    "\n",
    "        for outputs in all_outputs:\n",
    "            merged.append(func(outputs))\n",
    "\n",
    "    return Model(inputs=model.inputs, outputs=merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = make_parallel(model,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
